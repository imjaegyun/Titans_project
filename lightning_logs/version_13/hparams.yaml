lightweight_llm_cfg:
  model_name: meta-llama/Llama-3.2-1B
  lr: 1.0e-05
  use_lora: false
vit_eagle_cfg:
  model_name: google/vit-base-patch16-224
  freeze: true
trajectory_encoder_cfg:
  input_dim: 2
  hidden_dim: 128
base_llm_cfg:
  model_name: meta-llama/Llama-3.2-1B
  tokenizer_name: meta-llama/Llama-3.2-1B
  freeze: true
  use_lora: true
  hidden_size: 2048
titans_cfg:
  d_model: 2048
  memory_depth: 2
  decoder_layers: 2
  surprise_decay: 0.9
  momentum: 0.9
  forget_alpha: 0.1
optimizer_cfg:
  lr: 0.0001
  weight_decay: 0.01
